{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleModel(nn.Module):\n",
    "\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "\n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_ch = in_ch\n",
    "        self.out_ch = out_ch\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(self.in_ch, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, self.out_ch, bias = False),\n",
    "            nn.ReLU()\n",
    "        )(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 14"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = [1, 2, 1, 2, 4, 2, 1, 2, 1, -1, 0, 1, 0, 0, 0, 1, 0, -1, 0, 1, 0, 1, 2, 1, 0, 1, 0]\n",
    "img = torch.tensor(img, dtype = torch.float).reshape(3, 3, 3)\n",
    "\n",
    "img1 = [1, 0, 0, 1, 0, -1, -1, 0, 1, -1, 0, 0]\n",
    "img1 = torch.tensor(img1, dtype = torch.float).reshape(3, 2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-3.6121e-01,  1.9278e-01,  7.4678e-01],\n",
      "         [ 1.6323e-01,  2.4235e-01,  4.3452e-04],\n",
      "         [ 6.8768e-01, -2.9116e-02, -7.4591e-01]],\n",
      "\n",
      "        [[-1.3368e+00, -8.7429e-01, -4.1177e-01],\n",
      "         [-1.4875e+00, -7.3474e-01,  1.1528e-02],\n",
      "         [-1.6382e+00, -6.0167e-01,  4.3483e-01]],\n",
      "\n",
      "        [[-1.1343e+00, -9.7110e-01, -8.0787e-01],\n",
      "         [-3.4850e-01, -4.9166e-01, -1.4988e-01],\n",
      "         [ 4.3732e-01,  4.7271e-01,  5.0811e-01]]], grad_fn=<SqueezeBackward1>) \n",
      "\n",
      "tensor([[[-0.0524, -0.1676],\n",
      "         [-0.1687, -0.1032]],\n",
      "\n",
      "        [[-0.2046, -0.1020],\n",
      "         [ 0.4655,  0.1433]],\n",
      "\n",
      "        [[-0.2761,  0.1537],\n",
      "         [ 0.0610,  0.0037]]], grad_fn=<SqueezeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "conv = torch.nn.Conv2d(3, 3, kernel_size = 2, padding = 1, stride = 1, dilation = 2, bias = False)\n",
    "\n",
    "print(conv(img), '\\n')\n",
    "print(conv(img1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_receptive_field(layers_cnt, kernels_size, strides, paddings, dilations):\n",
    "\n",
    "    res, s = 1, 1\n",
    "\n",
    "    for l in range(layers_cnt):\n",
    "        for i in range(l):\n",
    "\n",
    "            s *= strides[i]\n",
    "            res += dilations[l] * (kernels_size[l] - 1) * s \n",
    "\n",
    "    start_id, end_id = 0, 0\n",
    "\n",
    "    for l in range(layers_cnt -1, -1, -1):\n",
    "\n",
    "        start_id = -paddings[l] + start_id * strides[l]\n",
    "        end_id = -paddings[l] + end_id * strides[l] + kernels_size[l] - 1\n",
    "\n",
    "    return res, start_id, end_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(590, -402116, -402090)\n"
     ]
    }
   ],
   "source": [
    "kernels_size = [7, 3, 2, 3, 3]\n",
    "strides = [1, 2, 2, 1, 3]\n",
    "paddings = [3, 1, 0, 100500, 28]\n",
    "dilations = [1, 1, 1, 2, 3]\n",
    "\n",
    "print(get_receptive_field(5, kernels_size, strides, paddings, dilations))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_size(input_size, kernel_size, padding, stride, dilation):\n",
    "\n",
    "    return int(1 + (input_size + 2 * padding - dilation * (kernel_size - 1) - 1) / stride)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output image size: ( 26 , 26 )\n"
     ]
    }
   ],
   "source": [
    "size = new_size(224, kernel_size = 7, padding = 3, stride = 2, dilation = 1)\n",
    "size = new_size(size, kernel_size = 3, padding = 1, stride = 1, dilation = 1)\n",
    "size = size / 2\n",
    "size = new_size(size, kernel_size = 3, padding = 1, stride = 2, dilation = 3)\n",
    "\n",
    "print('Output image size:', end = ' ')\n",
    "print('(', size, ',', size, ')')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "af874b8d4f4459788df716e07a9b2cbb6aef8352bcd428451245bb55fab53f6a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
